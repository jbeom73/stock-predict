# scripts/build_index.py
# -*- coding: utf-8 -*-
"""
Run ë²„íŠ¼ë§Œ ëˆ„ë¥´ë©´ ì¸ë±ìŠ¤ ë¹Œë“œ.
- ìœ ë‹ˆë²„ìŠ¤ ì„ íƒì€ íŒŒì¼ ìƒë‹¨ í† ê¸€ë¡œ ë³€ê²½
- ë¡œì»¬ ohlcv ì—†ìœ¼ë©´ (ì˜µì…˜) ìë™ fetch
- (ì˜µì…˜) features ìºì‹œ ì €ì¥
- 84D ì„ë² ë”© + FAISS ì¸ë±ìŠ¤ (ì‹ í˜• meta í¬ë§·)

ê²°ê³¼:
  index/{INDEX_NAME}.idx
  index/{INDEX_NAME}.meta.json   # {"normalized": true/false, "metric": "...", "items": [...]}
"""
from __future__ import annotations

from pathlib import Path
import sys
import traceback
import numpy as np
import pandas as pd

# ==========================
# ğŸ”§ USER TOGGLES (ì—¬ê¸°ë§Œ ë°”ê¿”ì„œ ì‚¬ìš©)
# ==========================
UNIVERSE_MODE = "KOSPI"   # "LIST" | "KOSPI" | "KOSDAQ" | "ALL"
LIST_TICKERS  = [         # UNIVERSE_MODE="LIST"ì¼ ë•Œë§Œ ì‚¬ìš©
    "005930","000660","006400","035420","035720","051910","068270","005380","000270","012330",
    "015760","096770","066570","028260","005490","086790","010950","034220","105560","000810",
    "055550","032830","017670","003550","018260","090430","011170","097950","034730","011780",
    "000120","003670","036570","251270","011200","010130","004020","006800","009540","042660",
    "267250","028050","241560","047810","271560","316140","005830","086520","247540","091990",
]

AUTO_FETCH_IF_MISSING = True     # ë¡œì»¬ì— ohlcv ì—†ìœ¼ë©´ pykrxë¡œ ê°€ì ¸ì™€ ì €ì¥
WRITE_FEATURES_CACHE = True      # data/features/{ticker}.parquet ê°™ì´ ì €ì¥

WINDOW    = 60
STEP      = 1
FWD_DAYS  = 20
VOL_MODE  = "zlog"               # "zlog" | "turnover_zlog" | "ratio"
INDEX_NAME = "W60_retstats_candle84"

# âœ… ì§€í‘œ ê³„ì‚°/ìœˆë„ìš° ìƒì„±ì„ ìœ„í•œ ìµœì†Œ ê¸¸ì´(ì´í•˜ ìŠ¤í‚µ)
#   - ma120, rsi14, atr14 ë“±ì„ ì•ˆì •ì ìœ¼ë¡œ ê³„ì‚°í•˜ë ¤ë©´ ~120+ ì—¬ìœ  í•„ìš”
MIN_REQUIRED_ROWS = 130
# ==========================

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ë³´ì •
ROOT_DIR = Path(__file__).resolve().parents[1]
if str(ROOT_DIR) not in sys.path:
    sys.path.insert(0, str(ROOT_DIR))

DATA_DIR    = ROOT_DIR / "data"
DATA_OHLCV  = DATA_DIR / "ohlcv"
DATA_FEAT   = DATA_DIR / "features"
INDEX_DIR   = ROOT_DIR / "index"

print("[cfg] ROOT_DIR  =", ROOT_DIR)
print("[cfg] INDEX_DIR =", INDEX_DIR)

# project modules
from pattern.data_loader import get_ticker_list, fetch_ohlcv
from pattern.features import add_features      # â† ATR14 ì•ˆì „ ê³„ì‚° ë²„ì „ ê¶Œì¥
from pattern.windows import make_windows
from pattern.embed_candle import embed_window_candle
from pattern.indexer import build_faiss

# ì„ë² ë”© ì…ë ¥ ì»¬ëŸ¼ (ì• 5ê°œ OHLCV ê³ ì •)
FEATURES = [
    "open","high","low","close","volume",
    "logv","ma5","ma20","ma60","ma120",
    "rsi14","mfi14","stochK","atr14","px_over_ma20",
]

# ---------- utils ----------
def _resolve_universe() -> list[str]:
    mode = UNIVERSE_MODE.upper().strip()
    if mode == "LIST":
        return [str(t).zfill(6) for t in LIST_TICKERS]
    if mode in ("KOSPI", "KOSDAQ", "ALL"):
        codes = get_ticker_list("ALL" if mode == "ALL" else mode)
        return [str(c).zfill(6) for c in codes]
    # fallback
    return [str(t).zfill(6) for t in LIST_TICKERS]

def _read_local_ohlcv(ticker: str) -> pd.DataFrame | None:
    """data/ohlcv/{ticker}.parquet ì½ê¸° â†’ DatetimeIndex ë³´ì¥."""
    DATA_OHLCV.mkdir(parents=True, exist_ok=True)
    fp = DATA_OHLCV / f"{ticker}.parquet"
    if not fp.exists():
        return None

    df = pd.read_parquet(fp)
    # ì¤‘ë³µ ì»¬ëŸ¼ ì œê±°
    df = df.loc[:, ~df.columns.duplicated(keep="first")]

    # 'date' ìŠ¹ê²© ì‹œë„
    if "date" in df.columns:
        df["date"] = pd.to_datetime(df["date"], errors="coerce")
        df = df.set_index("date")
    else:
        # í˜¸í™˜: í˜¹ì‹œ ë‹¤ë¥¸ ì´ë¦„ìœ¼ë¡œ ì €ì¥ëœ ê²½ìš°
        for cand in ("Date", "datetime", "Datetime", "time", "Time"):
            if cand in df.columns:
                df[cand] = pd.to_datetime(df[cand], errors="coerce")
                df = df.set_index(cand)
                break

    if not isinstance(df.index, pd.DatetimeIndex):
        return None

    return df.sort_index()

def _save_local_ohlcv(ticker: str, df: pd.DataFrame):
    """data/ohlcv/{ticker}.parquet ì €ì¥ (í•­ìƒ date 1ê°œ, index=False)."""
    DATA_OHLCV.mkdir(parents=True, exist_ok=True)
    out = df.copy()

    # ì¤‘ë³µ ì»¬ëŸ¼ ì œê±°
    out = out.loc[:, ~out.columns.duplicated(keep="first")]

    # ì¸ë±ìŠ¤ê°€ ë¬´ì—‡ì´ë“  'date' ì»¬ëŸ¼ì„ 1ê°œ ë§Œë“¤ê¸°(ë¬´ì¡°ê±´ reset_index)
    out = out.reset_index()
    first_col = out.columns[0]
    if first_col != "date":
        out = out.rename(columns={first_col: "date"})
    out["date"] = pd.to_datetime(out["date"], errors="coerce")

    # ì €ì¥
    (DATA_OHLCV / f"{ticker}.parquet").write_bytes(out.to_parquet(index=False))
    print(f"[ohlcv] saved {ticker}")

def _ensure_data(ticker: str) -> pd.DataFrame | None:
    df = _read_local_ohlcv(ticker)
    if df is not None and not df.empty:
        return df
    if not AUTO_FETCH_IF_MISSING:
        print(f"[skip] no local ohlcv: {ticker}")
        return None
    print(f"[fetch] {ticker} via KRX")
    df = fetch_ohlcv(ticker)
    if df is None or df.empty:
        print(f"[warn] empty: {ticker}")
        return None
    _save_local_ohlcv(ticker, df)
    return _read_local_ohlcv(ticker)

def _save_features_cache(ticker: str, feat: pd.DataFrame):
    """data/features/{ticker}.parquet ì €ì¥ (date 1ê°œë§Œ, index=False, ê²¬ê³ í™”)."""
    if not WRITE_FEATURES_CACHE:
        return

    DATA_FEAT.mkdir(parents=True, exist_ok=True)
    df = feat.copy()

    # 0) ì¤‘ë³µ ì»¬ëŸ¼ ì œê±°
    df = df.loc[:, ~df.columns.duplicated(keep="first")]

    # 1) date ì»¬ëŸ¼ ì •í™•íˆ 1ê°œë§Œ ë§Œë“¤ê¸° (ë¬´ì¡°ê±´ reset â†’ ì²« ì»¬ëŸ¼ëª…ì„ dateë¡œ)
    df = df.reset_index()
    first_col = df.columns[0]
    if first_col != "date":
        df = df.rename(columns={first_col: "date"})
    df["date"] = pd.to_datetime(df["date"], errors="coerce")

    # 2) dateë¥¼ í•­ìƒ ì²« ë²ˆì§¸ë¡œ
    cols = ["date"] + [c for c in df.columns if c != "date"]
    df = df[cols]

    # 3) ì €ì¥
    (DATA_FEAT / f"{ticker}.parquet").write_bytes(df.to_parquet(index=False))
    # print(f"[feat] saved {ticker}")

# ---------- main ----------
def main():
    tickers = _resolve_universe()
    print(f"[universe] {UNIVERSE_MODE}  count={len(tickers)}")

    all_emb, all_meta = [], []
    processed = skipped = 0

    for i, t in enumerate(tickers, start=1):
        try:
            df = _ensure_data(t)
            if df is None or df.empty:
                skipped += 1
                continue

            # âœ… ë„ˆë¬´ ì§§ì€ ì‹œê³„ì—´ì€ ìŠ¤í‚µ (ì§€í‘œ ê³„ì‚° ì•ˆì „ + ìœˆë„ìš° í™•ë³´)
            if len(df) < MIN_REQUIRED_ROWS:
                print(f"[skip] {t}: rows={len(df)} < MIN_REQUIRED_ROWS({MIN_REQUIRED_ROWS})")
                skipped += 1
                continue

            # ì§€í‘œ ìƒì„± (features.add_featuresëŠ” ë‚´ë¶€ì—ì„œ NaN ì²˜ë¦¬/ì•ˆì „ê³„ì‚°)
            feat = add_features(df)

            # í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸ (ì—†ìœ¼ë©´ ìŠ¤í‚µ)
            missing = [c for c in FEATURES if c not in feat.columns]
            if missing:
                print(f"[skip] {t}: missing features={missing}")
                skipped += 1
                continue

            # NaN êµ¬ê°„ ì œê±° í›„ ìºì‹œ ì €ì¥
            feat = feat.dropna(subset=FEATURES)
            if feat.empty:
                print(f"[skip] {t}: features all-NaN after drop")
                skipped += 1
                continue

            _save_features_cache(t, feat)

            # ìœˆë„ìš° í™•ë³´(ê³¼ê±° W + ë¯¸ë˜ FWD_DAYS)
            if len(feat) < WINDOW + FWD_DAYS:
                print(f"[skip] {t}: rows={len(feat)} < {WINDOW+FWD_DAYS}")
                skipped += 1
                continue

            X, y, meta = make_windows(
                feat, FEATURES, W=WINDOW, step=STEP, fwd_days=FWD_DAYS
            )
            if X is None or len(X) == 0:
                print(f"[skip] {t}: make_windows=0")
                skipped += 1
                continue

            # ì„ë² ë”© ìƒì„±
            for w, (s, e) in zip(X, meta):
                z = embed_window_candle(w, vol_mode=VOL_MODE).astype("float32")
                all_emb.append(z)
                all_meta.append((t, str(s.date()), str(e.date())))

            processed += 1
            if i % 25 == 0:
                print(f"  ...{i}/{len(tickers)} processed={processed} emb={len(all_emb)}")

        except Exception as ex:
            skipped += 1
            print(f"[err ] {t}: {ex.__class__.__name__}: {ex}")
            # ë””ë²„ê¹… í•„ìš” ì‹œ ì£¼ì„ í•´ì œ
            # traceback.print_exc()

    if not all_emb:
        raise RuntimeError("No embeddings created. Check data or FEATURES/min rows/calc errors.")

    emb = np.vstack(all_emb).astype("float32")
    INDEX_DIR.mkdir(parents=True, exist_ok=True)
    build_faiss(emb, all_meta, INDEX_NAME)

    print(f"[built] {INDEX_NAME} shape={emb.shape}")
    print(f"[done ] tickers processed={processed}  skipped={skipped}")
    print(f"[files] {INDEX_DIR / (INDEX_NAME + '.idx')} / {INDEX_DIR / (INDEX_NAME + '.meta.json')}")

if __name__ == "__main__":
    main()
